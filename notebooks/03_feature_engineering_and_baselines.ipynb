{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f63ca58-e50a-460d-8bec-1e3f70181116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build TF-IDF text features, reduce with TruncatedSVD, encode genres, run baseline clustering experiments (genres-only, text-only, combined).\n"
     ]
    }
   ],
   "source": [
    "# 03_feature_engineering_and_baselines\n",
    "print(\"Build TF-IDF text features, reduce with TruncatedSVD, encode genres, run baseline clustering experiments (genres-only, text-only, combined).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7345345c-0332-4a2e-b2ad-b91e34995061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cleaned data: (7787, 14)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "\n",
    "PROJECT_ROOT = Path(\"C:/Users/KIIT/OneDrive/Documents/Labmentix/netflix\")\n",
    "CLEANED_IN = PROJECT_ROOT / \"outputs\" / \"cleaned_netflix.csv\"\n",
    "OUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CLEANED_IN)\n",
    "print(\"Loaded cleaned data:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72053970-ac3d-49b2-b83e-76b35678f0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (7787, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2- Fit TF-IDF (safe default)\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words=\"english\", ngram_range=(1,2))\n",
    "corpus = df[\"description\"].fillna(\"\").astype(str).tolist()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "358603f9-fb62-4f4c-b88f-88af657a0c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD reduced text shape: (7787, 40)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "svd = TruncatedSVD(n_components=40, random_state=42)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "print(\"SVD reduced text shape:\", X_svd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8329e41e-c921-4944-93ba-5d2d93034333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique genres found: 42\n",
      "Genre multi-hot shape: (7787, 42)\n",
      "Saved MultiLabelBinarizer to outputs/mlb_encoder.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 (REPLACE with this defensive version)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from itertools import chain\n",
    "\n",
    "# Ensure 'genres_list' exists and is a proper list for every row\n",
    "if 'genres_list' not in df.columns:\n",
    "    if 'listed_in' in df.columns:\n",
    "        df['genres_list'] = df['listed_in'].fillna(\"\").astype(str).apply(lambda s: [x.strip() for x in s.split(\",\") if x.strip()])\n",
    "    else:\n",
    "        df['genres_list'] = [[] for _ in range(len(df))]\n",
    "\n",
    "# make sure each entry is a list\n",
    "def ensure_list_item(x):\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    s = str(x)\n",
    "    if s.startswith('[') and s.endswith(']'):\n",
    "        s = s.strip('[]')\n",
    "        return [p.strip().strip(\"'\\\"\") for p in s.split(',') if p.strip()]\n",
    "    return [p.strip() for p in s.split(',') if p.strip()]\n",
    "\n",
    "df['genres_list'] = df['genres_list'].apply(ensure_list_item)\n",
    "\n",
    "# Detect unique genres in dataset\n",
    "all_genres = sorted(set(chain.from_iterable(df['genres_list'])))\n",
    "print(\"Unique genres found:\", len(all_genres))\n",
    "if len(all_genres) == 0:\n",
    "    # No genres, produce an empty (n_samples, 0) array for compatibility\n",
    "    X_genres = np.zeros((df.shape[0], 0), dtype=float)\n",
    "    print(\"Warning: no genres found. X_genres will have shape\", X_genres.shape)\n",
    "else:\n",
    "    mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "    X_genres = mlb.fit_transform(df['genres_list'])\n",
    "    print(\"Genre multi-hot shape:\", X_genres.shape)\n",
    "    # optional: save the encoder\n",
    "    import joblib\n",
    "    joblib.dump(mlb, OUT_DIR / \"mlb_encoder.joblib\")\n",
    "    print(\"Saved MultiLabelBinarizer to outputs/mlb_encoder.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "647ce84c-45b6-4aa4-ae63-5892ffa68e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features scaled shape: (7787, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "scaler = StandardScaler()\n",
    "num_cols = [\"duration_num\", \"release_year\"]\n",
    "num_df = df[num_cols].fillna(df[num_cols].median())\n",
    "X_num = scaler.fit_transform(num_df)\n",
    "print(\"Numeric features scaled shape:\", X_num.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79e84e80-8375-4d6f-a5f9-5768120d8f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types/shapes before combine:\n",
      " - X_svd: <class 'numpy.ndarray'> (7787, 40)\n",
      " - X_genres: <class 'numpy.ndarray'> (7787, 42)\n",
      " - X_num: <class 'numpy.ndarray'> (7787, 2)\n",
      "✅ Combined feature matrix shape: (7787, 84)\n",
      "OSError while saving to C:\\Users\\KIIT\\OneDrive\\Documents\\Labmentix\\netflix\\outputs\\X_combined.npy : [Errno 22] Invalid argument: 'C:\\\\Users\\\\KIIT\\\\OneDrive\\\\Documents\\\\Labmentix\\\\netflix\\\\outputs\\\\X_combined.npy'\n",
      "Attempting fallback save to: C:\\temp\\X_combined.npy\n",
      "✅ Saved numpy array to: C:\\temp\\X_combined.npy\n",
      "Saved to fallback path. Consider using a local path instead of OneDrive to avoid sync/lock issues.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 (robust: combine features and save X_combined.npy safely)\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "\n",
    "OUT_DIR = Path(\"C:/Users/KIIT/OneDrive/Documents/Labmentix/netflix/outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "save_path = OUT_DIR / \"X_combined.npy\"\n",
    "\n",
    "# assume X_svd, X_genres, X_num exist from earlier cells\n",
    "# Defensive checks:\n",
    "print(\"Types/shapes before combine:\")\n",
    "print(\" - X_svd:\", type(X_svd), getattr(X_svd, \"shape\", None))\n",
    "print(\" - X_genres:\", type(X_genres), getattr(X_genres, \"shape\", None))\n",
    "print(\" - X_num:\", type(X_num), getattr(X_num, \"shape\", None))\n",
    "\n",
    "# Convert sparse inputs to dense if needed (but check memory)\n",
    "def to_dense_if_sparse(arr, max_bytes=1_000_000_000):\n",
    "    \"\"\"Convert sparse matrix to dense array if resulting size < max_bytes (default ~1GB).\"\"\"\n",
    "    if sp.issparse(arr):\n",
    "        n_bytes = arr.shape[0] * arr.shape[1] * 8  # approximate for float64\n",
    "        print(f\" - Detected sparse matrix, approx bytes if dense: {n_bytes:,}\")\n",
    "        if n_bytes > max_bytes:\n",
    "            raise MemoryError(f\"Converting sparse -> dense would use ~{n_bytes:,} bytes which exceeds threshold.\")\n",
    "        return arr.toarray()\n",
    "    return np.asarray(arr)\n",
    "\n",
    "# Ensure all components are dense numeric numpy arrays\n",
    "try:\n",
    "    X_svd_arr = to_dense_if_sparse(X_svd)\n",
    "    X_genres_arr = to_dense_if_sparse(X_genres)\n",
    "    X_num_arr = to_dense_if_sparse(X_num)\n",
    "except MemoryError as me:\n",
    "    print(\"MemoryError:\", me)\n",
    "    print(\"Try reducing feature sizes (e.g. fewer SVD components / fewer TF-IDF features) or save to disk incrementally.\")\n",
    "    raise\n",
    "\n",
    "# Ensure 2D arrays and compatible row counts\n",
    "for name, arr in [(\"X_svd\", X_svd_arr), (\"X_genres\", X_genres_arr), (\"X_num\", X_num_arr)]:\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(-1, 1)\n",
    "    if arr.shape[0] != X_svd_arr.shape[0]:\n",
    "        raise ValueError(f\"Row count mismatch: {name}.rows={arr.shape[0]} vs X_svd.rows={X_svd_arr.shape[0]}\")\n",
    "    # replace with possibly reshaped arr in local vars\n",
    "    if name == \"X_svd\":\n",
    "        X_svd_arr = arr\n",
    "    elif name == \"X_genres\":\n",
    "        X_genres_arr = arr\n",
    "    else:\n",
    "        X_num_arr = arr\n",
    "\n",
    "# Combine horizontally (dense)\n",
    "X_combined = np.hstack([X_svd_arr, X_genres_arr, X_num_arr])\n",
    "print(\"✅ Combined feature matrix shape:\", X_combined.shape)\n",
    "\n",
    "# Convert to a compact numeric dtype to reduce file size (safe: float32)\n",
    "if np.issubdtype(X_combined.dtype, np.floating):\n",
    "    X_to_save = X_combined.astype(np.float32, copy=False)\n",
    "else:\n",
    "    # try to coerce to float32; if fails, keep as object and warn\n",
    "    try:\n",
    "        X_to_save = X_combined.astype(np.float32)\n",
    "    except Exception:\n",
    "        X_to_save = X_combined\n",
    "        print(\"Warning: X_combined could not be cast to float32; it will be saved with existing dtype:\", X_combined.dtype)\n",
    "\n",
    "# --- Save robustly using a string path and file handle ---\n",
    "def safe_save_numpy(arr, path_obj):\n",
    "    path_str = str(path_obj)\n",
    "    try:\n",
    "        # use a file handle — more robust on Windows/OneDrive\n",
    "        with open(path_str, \"wb\") as f:\n",
    "            np.save(f, arr, allow_pickle=False)\n",
    "        print(\"✅ Saved numpy array to:\", path_str)\n",
    "        return True\n",
    "    except OSError as e:\n",
    "        print(\"OSError while saving to\", path_str, \":\", e)\n",
    "        return False\n",
    "\n",
    "success = safe_save_numpy(X_to_save, save_path)\n",
    "\n",
    "# If saving failed (OneDrive path issue), fall back to a local temp folder\n",
    "if not success:\n",
    "    fallback_dir = Path(\"C:/temp\")\n",
    "    fallback_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fallback_path = fallback_dir / \"X_combined.npy\"\n",
    "    print(\"Attempting fallback save to:\", fallback_path)\n",
    "    success2 = safe_save_numpy(X_to_save, fallback_path)\n",
    "    if success2:\n",
    "        print(\"Saved to fallback path. Consider using a local path instead of OneDrive to avoid sync/lock issues.\")\n",
    "    else:\n",
    "        raise OSError(\"Failed to save X_combined.npy to both primary and fallback locations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6667fb32-b462-4bc4-87c8-a122ebf9f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette [text] (k=5): 0.1540\n",
      "Silhouette [genres] (k=5): 0.1863\n",
      "Silhouette [combined] (k=5): 0.1855\n",
      "\n",
      "Baseline results summary: {'text': 0.1539922302062514, 'genres': 0.186302179232272, 'combined': 0.18551902271399665}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "baseline_results = {}\n",
    "# Prepare candidate feature sets\n",
    "candidates = [\n",
    "    (\"text\", X_svd),\n",
    "    (\"genres\", X_genres),\n",
    "    (\"combined\", X_combined)\n",
    "]\n",
    "\n",
    "for name, X in candidates:\n",
    "    # Ensure X is a NumPy array (dense) or sparse matrix that supports shape\n",
    "    if hasattr(X, \"shape\"):\n",
    "        n_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "    else:\n",
    "        X = np.asarray(X)\n",
    "        n_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "    if n_features == 0:\n",
    "        print(f\"SKIP baseline '{name}': feature matrix has 0 columns (shape={X.shape}).\")\n",
    "        baseline_results[name] = None\n",
    "        continue\n",
    "\n",
    "    # Defensive: if X is sparse, convert to array for KMeans (scikit-learn accepts sparse but silhouette may need dense)\n",
    "    try:\n",
    "        # If it's scipy sparse, KMeans accepts it but silhouette needs dense arrays for some versions;\n",
    "        # we'll convert to dense only if it's small enough\n",
    "        import scipy.sparse as sp\n",
    "        if sp.issparse(X):\n",
    "            # convert to dense if reasonable\n",
    "            if X.shape[0] * X.shape[1] <= 5_000_000:  # ~5M entries threshold (adjust if needed)\n",
    "                X_dense = X.toarray()\n",
    "            else:\n",
    "                # compute using sparse as-is (KMeans can accept sparse in newer sklearn)\n",
    "                X_dense = X\n",
    "        else:\n",
    "            X_dense = X\n",
    "    except Exception:\n",
    "        X_dense = X\n",
    "\n",
    "    # Fit KMeans with k=5 (ensure k < n_samples)\n",
    "    k_try = 5\n",
    "    if n_samples <= k_try:\n",
    "        k_try = max(2, n_samples // 2)\n",
    "        print(f\"Adjusted k for '{name}' to {k_try} because n_samples={n_samples}\")\n",
    "\n",
    "    km = KMeans(n_clusters=k_try, random_state=42)\n",
    "    labels = km.fit_predict(X_dense)\n",
    "\n",
    "    # Validate that we have at least 2 unique clusters for silhouette\n",
    "    unique_labels = np.unique(labels)\n",
    "    if unique_labels.size < 2:\n",
    "        print(f\"SKIP silhouette for '{name}': only {unique_labels.size} unique label(s) found.\")\n",
    "        baseline_results[name] = None\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        score = silhouette_score(X_dense, labels)\n",
    "        baseline_results[name] = float(score)\n",
    "        print(f\"Silhouette [{name}] (k={k_try}): {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute silhouette for '{name}': {e}\")\n",
    "        baseline_results[name] = None\n",
    "\n",
    "print(\"\\nBaseline results summary:\", baseline_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0caa915b-e7b1-493f-9575-fce85109fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TF-IDF, SVD, MLBin, and scaler to outputs/\n"
     ]
    }
   ],
   "source": [
    "# Cell 8\n",
    "joblib.dump(tfidf, OUT_DIR / \"tfidf_vectorizer.joblib\")\n",
    "joblib.dump(svd, OUT_DIR / \"svd_transformer.joblib\")\n",
    "joblib.dump(mlb, OUT_DIR / \"mlb_encoder.joblib\")\n",
    "joblib.dump(scaler, OUT_DIR / \"scaler.joblib\")\n",
    "print(\"Saved TF-IDF, SVD, MLBin, and scaler to outputs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c07ad-5c1a-4480-b8c4-8032dfc94a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
